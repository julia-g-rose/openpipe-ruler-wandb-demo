# Training configuration for email search agent
# This file defines all hyperparameters and settings for the training run

# Model configuration
project: "demo-project-qwen-email-agent-with-art-weave-models"
model_name_ruler: "qwen-trained-with-ruler"  # For RULER training
model_name_independent: "qwen-trained-with-independent-scorer-rewards"  # For independent reward training
model_name_combined: "qwen-trained-with-ruler-and-scorer-rewards"  # For combined RULER + independent rewards
base_model: "OpenPipe/Qwen3-14B-Instruct"

# Dataset configuration
training_dataset_size: 500  # Number of training scenarios to load (increased for better training)
validation_dataset_size: 100  # Number of validation scenarios to load
dataset_seed: 42  # Random seed for dataset shuffling

# Dataset artifacts
training_dataset_artifact: "enron-training-scenarios:latest"
validation_dataset_artifact: "enron-validation-scenarios:latest"

# Judge models
ruler_judge_model: "openai/gpt-5"  # Used by RULER for scoring trajectories during training
correctness_judge_model: "openai/gpt-5"  # Used for evaluating answer correctness
tool_judge_model: "openai/gpt-5"  # Used for evaluating tool call appropriateness

# Comparison models (for compare_models.py)
comparison_model: "gpt-5"  # Second model for leaderboard comparison

# Training hyperparameters
groups_per_step: 4  # Increased from 2 for more scenarios per update
num_epochs: 1 
rollouts_per_group: 4  # Balanced for RULER signal vs API reliability
learning_rate: 5.0e-6  # Decreased from 1.0e-5 for more stable updates

# Validation and evaluation
validation_step_interval: 25  # Decreased from 25 for more frequent validation
log_correctness_correlation_plots: true  # Log correlation plots comparing correct vs incorrect predictions

# Checkpointing
save_checkpoint_artifact: true  # Save checkpoints as W&B artifacts

# Reproducibility
random_seed: 42

# W&B run configuration
wandb_job_type: "train"

