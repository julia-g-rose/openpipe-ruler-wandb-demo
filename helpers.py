"""
Helper functions and models for the email search agent training and evaluation.
"""
import json
import logging
import yaml
from dataclasses import asdict
from pathlib import Path
from textwrap import dedent

import weave
from langchain_core.utils.function_calling import convert_to_openai_tool
from litellm import acompletion
from openai import AsyncOpenAI
from pydantic import BaseModel, Field
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential_jitter,
    retry_if_exception_type,
    before_sleep_log,
)
from openai import RateLimitError, APITimeoutError

import art
from art.utils.strip_logprobs import strip_logprobs
from enron_helpers import Scenario, FinalAnswer, search_emails, read_email


# Setup logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# Suppress weave validation errors
logging.getLogger("weave").setLevel(logging.CRITICAL)

# Decrease the number of turns from 10 to 6 to speed up training
MAX_TURNS = 6


# Weave Prompts for LLM Judges
# These prompts are versioned and tracked in Weave for reproducibility

CORRECTNESS_JUDGE_PROMPT = weave.StringPrompt(
    """You are given a question, the reference answer (labelled **Reference answer**), and an answer generated by an AI assistant (labelled **AI answer**).

Your task is to decide whether the AI answer is correct and should be accepted. You should accept the answer if it contains the relevant information from the reference answer. You should not accept the answer if it is missing information relevant to the question, or if it contradicts the reference answer."""
)

TOOL_CALL_JUDGE_PROMPT = weave.StringPrompt(
    """You are evaluating whether an AI agent made an appropriate tool call decision.

The agent has access to three tools:
1. search_inbox(keywords: list[str]) - Search for emails matching keywords
2. read_email(message_id: str) - Read the full content of a specific email
3. return_final_answer(answer: str, reference_message_ids: list[str]) - Return the final answer

Your task is to evaluate whether the tool call was appropriate given:
- The user's original query
- The conversation history so far
- The tool that was called and its arguments
- The result returned by the tool

Categorize the tool call as:
- "optimal": The best possible tool choice that makes clear progress toward the goal
- "suboptimal": A reasonable tool choice but not ideal (e.g., poor keyword selection, unnecessary reads)
- "incorrect": Wrong tool choice or arguments that don't help solve the task

Consider:
- Are the search keywords relevant to the query?
- Is the agent reading emails that are likely to contain the answer?
- Is the agent making redundant tool calls?
- Is the final answer being returned at an appropriate time?"""
)

NO_TOOL_CALL_JUDGE_PROMPT = weave.StringPrompt(
    """You are evaluating whether an AI agent made an appropriate decision by NOT calling a tool.

The agent has access to three tools:
1. search_inbox(keywords: list[str]) - Search for emails matching keywords
2. read_email(message_id: str) - Read the full content of a specific email
3. return_final_answer(answer: str, reference_message_ids: list[str]) - Return the final answer

The agent chose to respond with text instead of calling a tool. Your task is to evaluate whether this was appropriate given:
- The user's original query
- The conversation history so far
- The text response the agent provided

Categorize the decision as:
- "optimal": Not calling a tool was the best choice (e.g., the task is already complete, or clarification is needed)
- "suboptimal": Not ideal but acceptable (e.g., could have made progress with a tool call but the response isn't harmful)
- "incorrect": The agent should have called a tool (e.g., needs to search for emails, read specific emails, or return a final answer)

Consider:
- Does the agent need to search for more information?
- Should the agent be reading specific emails that were mentioned?
- Should the agent return a final answer with the return_final_answer tool?
- Is the agent stuck or making no progress toward solving the task?"""
)

EMAIL_AGENT_SYSTEM_PROMPT = weave.StringPrompt(
    """You are an email search agent. You are given a user query and a list of tools you can use to search the user's email. Use the tools to search the user's emails and find the answer to the user's query. You may take up to {max_turns} turns to find the answer, so if your first search doesn't find the answer, you can try with different keywords.

User's email address is {inbox_address}
Today's date is {query_date}"""
)


def load_config(config_path: str = "config.yaml") -> dict:
    """
    Load configuration from YAML file.
    
    Args:
        config_path: Path to the YAML configuration file
        
    Returns:
        Dictionary containing the configuration
        
    Raises:
        FileNotFoundError: If the config file doesn't exist
    """
    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
    
    return config


class CorrectnessJudgeResponse(BaseModel):
    reasoning: str = Field(description="Explanation of the reasoning process.")
    accept: bool = Field(description="Whether the AI answer should be accepted.")


class ToolUsageJudgeResponse(BaseModel):
    reasoning: str = Field(description="Explanation of whether the tool call was appropriate given the context.")
    label: str = Field(description="Label categorizing the tool call: 'optimal', 'suboptimal', or 'incorrect'.")


class CorrectnessJudgeScorer(weave.Scorer):
    """Weave Scorer for judging answer correctness using an LLM judge.
    
    This scorer evaluates whether an AI-generated answer is correct by comparing it
    to a reference answer using a judge LLM model.
    
    Attributes:
        judge_model: The LLM model to use for judging (default: openai/gpt-4o)
        max_retries: Maximum number of retry attempts (default: 3)
    """
    judge_model: str = "openai/gpt-4o"
    max_retries: int = 3
    
    @weave.op()
    async def score(
        self, 
        output: str, 
        question: str,
        reference_answer: str
    ) -> dict:
        """Score the output by judging its correctness against a reference answer.
        
        Args:
            output: The AI-generated answer to evaluate
            question: The original question being answered
            reference_answer: The reference/ground truth answer
            
        Returns:
            Dict containing:
                - correct: Boolean score (1.0 if accepted, 0.0 if rejected)
                - reasoning: Explanation of the judge's decision
                - accept: Whether the answer should be accepted
        """
        @retry(
            stop=stop_after_attempt(self.max_retries),
            wait=wait_exponential_jitter(initial=1, max=60, jitter=5),
            retry=retry_if_exception_type((RateLimitError, Exception)),
        )
        async def _judge_with_retry():
            # Use Weave-managed prompt
            system_prompt = CORRECTNESS_JUDGE_PROMPT.format()

            messages = [
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": (
                        f"Question: {question}\n"
                        f"Reference answer: {reference_answer}\n"
                        f"AI answer: {output}"
                    ),
                },
            ]

            response = await acompletion(
                model=self.judge_model,
                messages=messages,
                response_format=CorrectnessJudgeResponse,
            )

            first_choice = response.choices[0]
            raw_content = first_choice.message.content or "{}"

            try:
                return CorrectnessJudgeResponse.model_validate_json(raw_content)
            except Exception as e:
                return CorrectnessJudgeResponse(
                    reasoning=f"Parse error: {e}\nRaw: {raw_content}", accept=False
                )
        
        try:
            judge_response = await _judge_with_retry()
            return {
                "correct": float(judge_response.accept),
                "reasoning": judge_response.reasoning,
                "accept": judge_response.accept
            }
        except Exception as e:
            # If all retries fail, return a failed judgment
            return {
                "correct": 0.0,
                "reasoning": f"Failed to get judgment after {self.max_retries} attempts: {str(e)}",
                "accept": False
            }


class SourceRetrievalScorer(weave.Scorer):
    """Weave Scorer for evaluating whether the agent retrieved the correct source emails.
    
    This scorer checks if the agent's final answer references the correct source
    email IDs that were expected for the scenario. It evaluates the quality of
    the agent's search and retrieval strategy.
    
    Metrics returned:
        - retrieved_correct_sources: Whether all expected sources were retrieved
    """
    
    @weave.op()
    async def score(
        self,
        output: dict,  # Should contain 'source_ids' key
        expected_source_ids: list[str]
    ) -> dict:
        """Score the source retrieval quality.
        
        Args:
            output: Dict containing the agent's output, must have 'source_ids' key
            expected_source_ids: List of expected message IDs that should be retrieved
            
        Returns:
            Dict containing retrieved_correct_sources metric
        """
        # Extract source IDs from output
        if isinstance(output, dict):
            retrieved_ids = set(output.get('source_ids', []))
        elif hasattr(output, 'source_ids'):
            retrieved_ids = set(output.source_ids)
        else:
            # No sources retrieved
            retrieved_ids = set()
        
        expected_ids = set(expected_source_ids)
        
        # Check if all expected sources were retrieved
        if len(expected_ids) == 0:
            retrieved_all = len(retrieved_ids) == 0
        else:
            correct_retrievals = retrieved_ids.intersection(expected_ids)
            retrieved_all = len(correct_retrievals) == len(expected_ids)
        
        return {
            "retrieved_correct_sources": float(retrieved_all)
        }


class ToolUsageScorer(weave.Scorer):
    """Weave Scorer for evaluating individual tool call decisions using an LLM judge.
    
    This scorer uses an LLM to evaluate whether each tool call made by the agent was
    appropriate given the conversation context, the user's query, and the task at hand.
    
    Available tools: search_inbox, read_email, return_final_answer
    
    The judge evaluates:
    - Whether the tool choice was appropriate for the current situation
    - Whether the tool arguments (e.g., search keywords) were well-chosen
    - Whether the tool call represents progress toward solving the task
    
    Attributes:
        judge_model: The LLM model to use for judging (default: openai/gpt-4o)
        max_retries: Maximum number of retry attempts (default: 3)
    """
    judge_model: str = "openai/gpt-4o"
    max_retries: int = 3
    
    @weave.op()
    async def score(
        self,
        conversation_history: list,
        tool_call: dict,
        tool_result: str,
        user_query: str
    ) -> dict:
        """Score a single tool call decision.
        
        Args:
            conversation_history: List of messages leading up to this tool call
            tool_call: The tool call to evaluate (dict with 'name' and 'arguments')
            tool_result: The result returned by the tool
            user_query: The original user query/question
            
        Returns:
            Dict containing:
                - label: Categorization ('optimal', 'suboptimal', or 'incorrect')
                - reasoning: Explanation of the judgment
        """
        @retry(
            stop=stop_after_attempt(self.max_retries),
            wait=wait_exponential_jitter(initial=1, max=60, jitter=5),
            retry=retry_if_exception_type((RateLimitError, Exception)),
        )
        async def _judge_with_retry():
            # Extract tool call information
            if isinstance(tool_call, dict):
                tool_name = tool_call.get('function', {}).get('name', '') or tool_call.get('name', '')
                tool_args = tool_call.get('function', {}).get('arguments', '') or tool_call.get('arguments', '')
            else:
                tool_name = getattr(tool_call, 'name', '')
                tool_args = getattr(tool_call, 'arguments', '')
            
            # Format conversation history for the judge
            history_str = ""
            for msg in conversation_history[-5:]:  # Last 5 messages for context
                if isinstance(msg, dict):
                    role = msg.get('role', 'unknown')
                    content = msg.get('content', '')
                    if role == 'system':
                        history_str += f"[SYSTEM]: {content[:200]}...\n" if len(content) > 200 else f"[SYSTEM]: {content}\n"
                    elif role == 'user':
                        history_str += f"[USER]: {content}\n"
                    elif role == 'assistant':
                        history_str += f"[ASSISTANT]: {content}\n"
                    elif role == 'tool':
                        tool_name_msg = msg.get('name', 'unknown')
                        history_str += f"[TOOL-{tool_name_msg}]: {content[:200]}...\n" if len(content) > 200 else f"[TOOL-{tool_name_msg}]: {content}\n"
            
            # Use Weave-managed prompt
            system_prompt = TOOL_CALL_JUDGE_PROMPT.format()
            
            user_message = dedent(
                f"""
                **User Query:** {user_query}
                
                **Conversation History:**
                {history_str}
                
                **Tool Call Made:**
                Tool: {tool_name}
                Arguments: {tool_args}
                
                **Tool Result:**
                {tool_result[:500]}{'...' if len(tool_result) > 500 else ''}
                
                Evaluate whether this tool call was appropriate.
                """
            )
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message},
            ]
            
            response = await acompletion(
                model=self.judge_model,
                messages=messages,
                response_format=ToolUsageJudgeResponse,
            )
            
            first_choice = response.choices[0]
            raw_content = first_choice.message.content or "{}"
            
            try:
                return ToolUsageJudgeResponse.model_validate_json(raw_content)
            except Exception as e:
                return ToolUsageJudgeResponse(
                    reasoning=f"Parse error: {e}\nRaw: {raw_content}",
                    label="incorrect"
                )
        
        try:
            judge_response = await _judge_with_retry()
            return {
                "label": judge_response.label,
                "reasoning": judge_response.reasoning
            }
        except Exception as e:
            return {
                "label": "error",
                "reasoning": f"Failed to get judgment after {self.max_retries} attempts: {str(e)}"
            }
    
    @weave.op()
    async def score_no_tool_call(
        self,
        conversation_history: list,
        assistant_response: str,
        user_query: str
    ) -> dict:
        """Score when the agent chose NOT to make a tool call.
        
        Args:
            conversation_history: List of messages leading up to this response
            assistant_response: The text response the agent provided instead of calling a tool
            user_query: The original user query/question
            
        Returns:
            Dict containing:
                - label: Categorization ('optimal', 'suboptimal', or 'incorrect')
                - reasoning: Explanation of the judgment
        """
        @retry(
            stop=stop_after_attempt(self.max_retries),
            wait=wait_exponential_jitter(initial=1, max=60, jitter=5),
            retry=retry_if_exception_type((RateLimitError, Exception)),
        )
        async def _judge_with_retry():
            # Format conversation history for the judge
            history_str = ""
            for msg in conversation_history[-5:]:  # Last 5 messages for context
                if isinstance(msg, dict):
                    role = msg.get('role', 'unknown')
                    content = msg.get('content', '')
                    if role == 'system':
                        history_str += f"[SYSTEM]: {content[:200]}...\n" if len(content) > 200 else f"[SYSTEM]: {content}\n"
                    elif role == 'user':
                        history_str += f"[USER]: {content}\n"
                    elif role == 'assistant':
                        history_str += f"[ASSISTANT]: {content}\n"
                    elif role == 'tool':
                        tool_name_msg = msg.get('name', 'unknown')
                        history_str += f"[TOOL-{tool_name_msg}]: {content[:200]}...\n" if len(content) > 200 else f"[TOOL-{tool_name_msg}]: {content}\n"
            
            # Use Weave-managed prompt
            system_prompt = NO_TOOL_CALL_JUDGE_PROMPT.format()
            
            user_message = dedent(
                f"""
                **User Query:** {user_query}
                
                **Conversation History:**
                {history_str}
                
                **Agent's Decision:**
                The agent chose NOT to call any tool and instead responded with: "{assistant_response[:300]}{'...' if len(assistant_response) > 300 else ''}"
                
                Evaluate whether this decision (not calling a tool) was appropriate.
                """
            )
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message},
            ]
            
            response = await acompletion(
                model=self.judge_model,
                messages=messages,
                response_format=ToolUsageJudgeResponse,
            )
            
            first_choice = response.choices[0]
            raw_content = first_choice.message.content or "{}"
            
            try:
                return ToolUsageJudgeResponse.model_validate_json(raw_content)
            except Exception as e:
                return ToolUsageJudgeResponse(
                    reasoning=f"Parse error: {e}\nRaw: {raw_content}",
                    label="incorrect"
                )
        
        try:
            judge_response = await _judge_with_retry()
            return {
                "label": judge_response.label,
                "reasoning": judge_response.reasoning
            }
        except Exception as e:
            return {
                "label": "error",
                "reasoning": f"Failed to get judgment after {self.max_retries} attempts: {str(e)}"
            }


class ProjectTrajectory(art.Trajectory):
    final_answer: FinalAnswer | None = None
    tool_evaluations: list[dict] = []  # Store LLM judge evaluations for each tool call


class EmailScenario(BaseModel):
    step: int
    scenario: Scenario


def _add_tool_usage_metrics(traj: ProjectTrajectory):
    """Add aggregate tool usage metrics from individual tool evaluations.
    
    Args:
        traj: The trajectory to add metrics to
    """
    if not traj.tool_evaluations:
        traj.metrics["tool_optimal_rate"] = 0.0
        return
    
    # Count tool calls by optimality
    total = len(traj.tool_evaluations)
    optimal_count = sum(1 for eval in traj.tool_evaluations if eval["label"] == "optimal")
    
    # Add aggregate metrics
    traj.metrics["tool_optimal_rate"] = optimal_count / total if total > 0 else 0.0


@weave.op
async def rollout(
    model: art.Model, 
    email_scenario: EmailScenario,
    correctness_judge_model: str = "openai/gpt-4o",
    tool_judge_model: str = "openai/gpt-4o"
) -> ProjectTrajectory:
    """
    Execute a rollout of the email search agent on a given scenario.
    
    Args:
        model: The ART model to use for inference
        email_scenario: The email scenario to process
        correctness_judge_model: The model to use for judging answer correctness
        tool_judge_model: The model to use for judging tool call appropriateness
        
    Returns:
        A ProjectTrajectory containing the agent's conversation and results
    """
    scenario = email_scenario.scenario

    traj = ProjectTrajectory(
        reward=0.0,
        messages_and_choices=[],
        metadata={
            "scenario_id": scenario.id,
            "step": email_scenario.step,
        },
    )

    # Use Weave-managed prompt with scenario-specific values
    system_prompt = EMAIL_AGENT_SYSTEM_PROMPT.format(
        max_turns=MAX_TURNS,
        inbox_address=scenario.inbox_address,
        query_date=scenario.query_date
    )

    traj.messages_and_choices = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": scenario.question},
    ]

    def search_inbox(keywords: list[str]) -> list[dict]:
        """Search the inbox for emails matching the given keywords and return
        a list of dictionaries so the LLM can easily consume them."""
        results = search_emails(
            inbox=scenario.inbox_address,
            keywords=keywords,
            sent_before=scenario.query_date,
        )
        return [asdict(result) for result in results]

    def return_final_answer(
        answer: str, reference_message_ids: list[str]
    ) -> FinalAnswer:
        """Return the final answer and the message IDs of the emails that were used to generate the answer."""
        return FinalAnswer(answer=answer, source_ids=reference_message_ids)

    tools = [search_inbox, read_email, return_final_answer]
    tools_by_name = {t.__name__: t for t in tools}
    traj.tools = [convert_to_openai_tool(t) for t in tools]

    # Create OpenAI client with reasonable timeouts to prevent indefinite hangs
    from httpx import Timeout
    client = AsyncOpenAI(
        base_url=model.inference_base_url,
        api_key=model.inference_api_key,
        timeout=Timeout(300.0, connect=60.0),  # 5 min total timeout, 60s connect timeout
    )
    
    # Initialize tool usage scorer for evaluating each tool call
    tool_scorer = ToolUsageScorer(judge_model=tool_judge_model)

    # Helper function with retry logic for API calls
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential_jitter(initial=1, max=60, jitter=5),
        retry=retry_if_exception_type((RateLimitError, APITimeoutError)),
        before_sleep=before_sleep_log(logger, logging.WARNING),
    )
    async def call_model_with_retry(force_final_answer: bool = False):
        """Call the model with retry logic for rate limit errors.
        
        Args:
            force_final_answer: If True, force the model to call return_final_answer
        """
        kwargs = {
            "model": model.get_inference_name(),
            "temperature": 1,
            "messages": traj.messages(),
            "tools": traj.tools,
        }
        
        # Force return_final_answer on the last turn if we haven't gotten an answer yet
        if force_final_answer:
            kwargs["tool_choice"] = {
                "type": "function",
                "function": {"name": "return_final_answer"}
            }
        
        return await client.chat.completions.create(**kwargs)

    for turn_num in range(MAX_TURNS):
        # Force final answer on the last turn if we haven't gotten one yet
        is_last_turn = (turn_num == MAX_TURNS - 1)
        force_answer = is_last_turn and traj.final_answer is None
        
        response = await call_model_with_retry(force_final_answer=force_answer)

        response_message = response.choices[0].message
        traj.messages_and_choices.append(response.choices[0])

        if not response_message.tool_calls:
            # Evaluate the decision to NOT make a tool call
            assistant_text = response_message.content or ""
            no_tool_evaluation = await tool_scorer.score_no_tool_call(
                conversation_history=traj.messages(),
                assistant_response=assistant_text,
                user_query=scenario.question
            )
            
            # Store the evaluation
            traj.tool_evaluations.append({
                "tool_name": "NO_TOOL_CALL",
                "tool_args": f"text_response: {assistant_text[:100]}",
                "label": no_tool_evaluation["label"],
                "reasoning": no_tool_evaluation["reasoning"]
            })
            
            # Add metrics and return
            _add_tool_usage_metrics(traj)
            return traj

        try:
            for tool_call in response_message.tool_calls:
                tool_name: str = tool_call.function.name
                if tool_name in tools_by_name:
                    tool_args = json.loads(tool_call.function.arguments)
                    tool_to_call = tools_by_name[tool_name]
                    result = tool_to_call(**tool_args)
                    result_str = str(result)
                    
                    traj.messages_and_choices.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "name": tool_name,
                            "content": result_str,
                        }
                    )
                    
                    # Evaluate this tool call using the LLM judge
                    tool_evaluation = await tool_scorer.score(
                        conversation_history=traj.messages(),
                        tool_call={
                            "name": tool_name,
                            "arguments": tool_call.function.arguments
                        },
                        tool_result=result_str,
                        user_query=scenario.question
                    )
                    
                    # Store the evaluation with metadata
                    traj.tool_evaluations.append({
                        "tool_name": tool_name,
                        "tool_args": tool_call.function.arguments,
                        "label": tool_evaluation["label"],
                        "reasoning": tool_evaluation["reasoning"]
                    })

                    if tool_name == "return_final_answer":
                        traj.final_answer = result
                        # Score the trajectory using Weave scorers
                        if traj.final_answer:
                            # Score answer correctness
                            correctness_scorer = CorrectnessJudgeScorer(judge_model=correctness_judge_model)
                            correctness_result = await correctness_scorer.score(
                                output=traj.final_answer.answer,
                                question=scenario.question,
                                reference_answer=scenario.answer
                            )
                            traj.metrics["correct"] = correctness_result["correct"]
                            # Store reasoning in metadata (not metrics) since it's a string
                            traj.metadata["judge_reasoning"] = correctness_result["reasoning"]
                            
                            # Score source retrieval quality
                            source_scorer = SourceRetrievalScorer()
                            source_result = await source_scorer.score(
                                output={"source_ids": traj.final_answer.source_ids},
                                expected_source_ids=scenario.message_ids
                            )
                            traj.metrics["retrieved_correct_sources"] = source_result["retrieved_correct_sources"]
                        
                        # Add aggregate tool usage metrics
                        _add_tool_usage_metrics(traj)
                        return traj
        except Exception as e:
            print(f"Error executing tool call: {e}")
            _add_tool_usage_metrics(traj)
            return traj

    # If we ran out of turns without a final answer, make one final forced attempt
    if traj.final_answer is None:
        try:
            print(f"⚠️  Reached max turns without final answer. Making forced final attempt...")
            # Add a user message prompting for final answer
            traj.messages_and_choices.append({
                "role": "user",
                "content": "Please provide your final answer now using the return_final_answer function."
            })
            
            response = await call_model_with_retry(force_final_answer=True)
            response_message = response.choices[0].message
            traj.messages_and_choices.append(response.choices[0])
            
            # Process the forced final answer
            if response_message.tool_calls:
                for tool_call in response_message.tool_calls:
                    tool_name = tool_call.function.name
                    if tool_name == "return_final_answer" and tool_name in tools_by_name:
                        tool_args = json.loads(tool_call.function.arguments)
                        tool_to_call = tools_by_name[tool_name]
                        result = tool_to_call(**tool_args)
                        traj.final_answer = result
                        
                        # Add the tool response message
                        traj.messages_and_choices.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "name": tool_name,
                            "content": str(result),
                        })
                        
                        # Evaluate the forced tool call
                        tool_evaluation = await tool_scorer.score(
                            conversation_history=traj.messages(),
                            tool_call={
                                "name": tool_name,
                                "arguments": tool_call.function.arguments
                            },
                            tool_result=str(result),
                            user_query=scenario.question
                        )
                        
                        traj.tool_evaluations.append({
                            "tool_name": tool_name,
                            "tool_args": tool_call.function.arguments,
                            "label": tool_evaluation["label"],
                            "reasoning": tool_evaluation["reasoning"]
                        })
                        
                        # Score the final answer
                        if traj.final_answer:
                            correctness_scorer = CorrectnessJudgeScorer(judge_model=correctness_judge_model)
                            correctness_result = await correctness_scorer.score(
                                output=traj.final_answer.answer,
                                question=scenario.question,
                                reference_answer=scenario.answer
                            )
                            traj.metrics["correct"] = correctness_result["correct"]
                            traj.metadata["judge_reasoning"] = correctness_result["reasoning"]
                            
                            source_scorer = SourceRetrievalScorer()
                            source_result = await source_scorer.score(
                                output={"source_ids": traj.final_answer.source_ids},
                                expected_source_ids=scenario.message_ids
                            )
                            traj.metrics["retrieved_correct_sources"] = source_result["retrieved_correct_sources"]
                        break
        except Exception as e:
            print(f"Error in forced final answer attempt: {e}")
    
    # Add aggregate tool usage metrics even if task wasn't completed (ran out of turns)
    _add_tool_usage_metrics(traj)
    return traj


def initialize_weave(project_name: str):
    """Initialize Weave with common settings and publish prompts."""
    weave.init(
        project_name,
        settings={"print_call_link": False},
        # remove logprobs before recording in Weave
        global_postprocess_output=strip_logprobs
    )
    
    # Publish prompts to Weave for version tracking with explicit names
    weave.publish(CORRECTNESS_JUDGE_PROMPT, name="correctness-judge-prompt")
    weave.publish(TOOL_CALL_JUDGE_PROMPT, name="tool-call-judge-prompt")
    weave.publish(NO_TOOL_CALL_JUDGE_PROMPT, name="no-tool-call-judge-prompt")
    weave.publish(EMAIL_AGENT_SYSTEM_PROMPT, name="email-agent-system-prompt")


def print_trajectory(trajectory: ProjectTrajectory, scenario: Scenario):
    """
    Pretty print a trajectory's messages and final answer.
    
    Args:
        trajectory: The trajectory to print
        scenario: The original scenario for comparison
    """
    print("Agent's trajectory:")
    print("-" * 20)

    # Display the conversation
    messages = trajectory.messages()
    for i, msg in enumerate(messages):
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        tool_calls = msg.get("tool_calls", [])

        if role == "system":
            print(
                f"[SYSTEM]: {content[:100]}..."
                if len(content) > 100
                else f"[SYSTEM]: {content}"
            )
        elif role == "user":
            print(f"[USER]: {content}")
        elif role == "assistant":
            if tool_calls:
                print(f"[ASSISTANT]: {tool_calls}")
            if content:
                print(f"[ASSISTANT]: {content}")
        elif role == "tool":
            tool_name = msg.get("name", "unknown_tool")
            print(
                f"[TOOL - {tool_name}]: {content[:200]}..."
                if len(content) > 200
                else f"[TOOL - {tool_name}]: {content}"
            )

        print()

    print("-" * 50)
    if trajectory.final_answer:
        print(f"Agent's Final Answer: {trajectory.final_answer.answer}")
        print(f"Source IDs Used: {trajectory.final_answer.source_ids}")
    else:
        print("No final answer provided by the agent")

    print(f"\nExpected Answer: {scenario.answer}")
    print(f"Expected Source IDs: {scenario.message_ids}")

