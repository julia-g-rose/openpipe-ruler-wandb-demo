"""
Helper functions and models for the email search agent training and evaluation.
"""
import json
import logging
import yaml
from dataclasses import asdict
from pathlib import Path
from textwrap import dedent

import weave
from langchain_core.utils.function_calling import convert_to_openai_tool
from litellm import acompletion
from openai import AsyncOpenAI
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt

import art
from art.utils.strip_logprobs import strip_logprobs
from enron_helpers import Scenario, FinalAnswer, search_emails, read_email


# Suppress weave validation errors
logging.getLogger("weave").setLevel(logging.CRITICAL)

# Decrease the number of turns from 10 to 6 to speed up training
MAX_TURNS = 6


def load_config(config_path: str = "config.yaml") -> dict:
    """
    Load configuration from YAML file.
    
    Args:
        config_path: Path to the YAML configuration file
        
    Returns:
        Dictionary containing the configuration
        
    Raises:
        FileNotFoundError: If the config file doesn't exist
    """
    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
    
    return config


class CorrectnessJudgeResponse(BaseModel):
    reasoning: str = Field(description="Explanation of the reasoning process.")
    accept: bool = Field(description="Whether the AI answer should be accepted.")


class CorrectnessJudgeScorer(weave.Scorer):
    """Weave Scorer for judging answer correctness using an LLM judge.
    
    This scorer evaluates whether an AI-generated answer is correct by comparing it
    to a reference answer using a judge LLM model.
    
    Attributes:
        judge_model: The LLM model to use for judging (default: openai/gpt-4.1)
        max_retries: Maximum number of retry attempts (default: 3)
    """
    judge_model: str = "openai/gpt-4.1"
    max_retries: int = 3
    
    @weave.op()
    async def score(
        self, 
        output: str, 
        question: str,
        reference_answer: str
    ) -> dict:
        """Score the output by judging its correctness against a reference answer.
        
        Args:
            output: The AI-generated answer to evaluate
            question: The original question being answered
            reference_answer: The reference/ground truth answer
            
        Returns:
            Dict containing:
                - correct: Boolean score (1.0 if accepted, 0.0 if rejected)
                - reasoning: Explanation of the judge's decision
                - accept: Whether the answer should be accepted
        """
        @retry(stop=stop_after_attempt(self.max_retries))
        async def _judge_with_retry():
            system_prompt = dedent(
                """
                You are given a question, the reference answer (labelled **Reference answer**), and an answer generated by an AI assistant (labelled **AI answer**).

                Your task is to decide whether the AI answer is correct and should be accepted. You should accept the answer if it contains the relevant information from the reference answer. You should not accept the answer if it is missing information relevant to the question, or if it contradicts the reference answer.
                """
            )

            messages = [
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": (
                        f"Question: {question}\n"
                        f"Reference answer: {reference_answer}\n"
                        f"AI answer: {output}"
                    ),
                },
            ]

            response = await acompletion(
                model=self.judge_model,
                messages=messages,
                response_format=CorrectnessJudgeResponse,
            )

            first_choice = response.choices[0]
            raw_content = first_choice.message.content or "{}"

            try:
                return CorrectnessJudgeResponse.model_validate_json(raw_content)
            except Exception as e:
                return CorrectnessJudgeResponse(
                    reasoning=f"Parse error: {e}\nRaw: {raw_content}", accept=False
                )
        
        try:
            judge_response = await _judge_with_retry()
            return {
                "correct": float(judge_response.accept),
                "reasoning": judge_response.reasoning,
                "accept": judge_response.accept
            }
        except Exception as e:
            # If all retries fail, return a failed judgment
            return {
                "correct": 0.0,
                "reasoning": f"Failed to get judgment after {self.max_retries} attempts: {str(e)}",
                "accept": False
            }


class ProjectTrajectory(art.Trajectory):
    final_answer: FinalAnswer | None = None


class EmailScenario(BaseModel):
    step: int
    scenario: Scenario


@weave.op
async def rollout(
    model: art.Model, 
    email_scenario: EmailScenario,
    correctness_judge_model: str = "openai/gpt-4.1"
) -> ProjectTrajectory:
    """
    Execute a rollout of the email search agent on a given scenario.
    
    Args:
        model: The ART model to use for inference
        email_scenario: The email scenario to process
        correctness_judge_model: The model to use for judging answer correctness
        
    Returns:
        A ProjectTrajectory containing the agent's conversation and results
    """
    scenario = email_scenario.scenario

    traj = ProjectTrajectory(
        reward=0.0,
        messages_and_choices=[],
        metadata={
            "scenario_id": scenario.id,
            "step": email_scenario.step,
        },
    )

    system_prompt = dedent(
        f"""
        You are an email search agent. You are given a user query and a list of tools you can use to search the user's email. Use the tools to search the user's emails and find the answer to the user's query. You may take up to {MAX_TURNS} turns to find the answer, so if your first search doesn't find the answer, you can try with different keywords.

        User's email address is {scenario.inbox_address}
        Today's date is {scenario.query_date}
        """
    )

    traj.messages_and_choices = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": scenario.question},
    ]

    def search_inbox(keywords: list[str]) -> list[dict]:
        """Search the inbox for emails matching the given keywords and return
        a list of dictionaries so the LLM can easily consume them."""
        results = search_emails(
            inbox=scenario.inbox_address,
            keywords=keywords,
            sent_before=scenario.query_date,
        )
        return [asdict(result) for result in results]

    def return_final_answer(
        answer: str, reference_message_ids: list[str]
    ) -> FinalAnswer:
        """Return the final answer and the message IDs of the emails that were used to generate the answer."""
        return FinalAnswer(answer=answer, source_ids=reference_message_ids)

    tools = [search_inbox, read_email, return_final_answer]
    tools_by_name = {t.__name__: t for t in tools}
    traj.tools = [convert_to_openai_tool(t) for t in tools]

    client = AsyncOpenAI(
        base_url=model.inference_base_url,
        api_key=model.inference_api_key,
    )

    for _ in range(MAX_TURNS):
        response = await client.chat.completions.create(
            model=model.get_inference_name(),
            temperature=1,
            messages=traj.messages(),
            tools=traj.tools,
        )

        response_message = response.choices[0].message
        traj.messages_and_choices.append(response.choices[0])

        if not response_message.tool_calls:
            return traj

        try:
            for tool_call in response_message.tool_calls:
                tool_name: str = tool_call.function.name
                if tool_name in tools_by_name:
                    tool_args = json.loads(tool_call.function.arguments)
                    tool_to_call = tools_by_name[tool_name]
                    result = tool_to_call(**tool_args)
                    traj.messages_and_choices.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "name": tool_name,
                            "content": str(result),
                        }
                    )

                    if tool_name == "return_final_answer":
                        traj.final_answer = result
                        # Score the trajectory using the Weave scorer
                        if traj.final_answer:
                            scorer = CorrectnessJudgeScorer(judge_model=correctness_judge_model)
                            score_result = await scorer.score(
                                output=traj.final_answer.answer,
                                question=scenario.question,
                                reference_answer=scenario.answer
                            )
                            traj.metrics["correct"] = score_result["correct"]
                            traj.metrics["reasoning"] = score_result["reasoning"]
                        return traj
        except Exception as e:
            print(f"Error executing tool call: {e}")
            return traj

    return traj


def initialize_weave(project_name: str):
    """Initialize Weave with common settings."""
    weave.init(
        project_name,
        settings={"print_call_link": False},
        # remove logprobs before recording in Weave
        global_postprocess_output=strip_logprobs
    )


def print_trajectory(trajectory: ProjectTrajectory, scenario: Scenario):
    """
    Pretty print a trajectory's messages and final answer.
    
    Args:
        trajectory: The trajectory to print
        scenario: The original scenario for comparison
    """
    print("Agent's trajectory:")
    print("-" * 20)

    # Display the conversation
    messages = trajectory.messages()
    for i, msg in enumerate(messages):
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        tool_calls = msg.get("tool_calls", [])

        if role == "system":
            print(
                f"[SYSTEM]: {content[:100]}..."
                if len(content) > 100
                else f"[SYSTEM]: {content}"
            )
        elif role == "user":
            print(f"[USER]: {content}")
        elif role == "assistant":
            if tool_calls:
                print(f"[ASSISTANT]: {tool_calls}")
            if content:
                print(f"[ASSISTANT]: {content}")
        elif role == "tool":
            tool_name = msg.get("name", "unknown_tool")
            print(
                f"[TOOL - {tool_name}]: {content[:200]}..."
                if len(content) > 200
                else f"[TOOL - {tool_name}]: {content}"
            )

        print()

    print("-" * 50)
    if trajectory.final_answer:
        print(f"Agent's Final Answer: {trajectory.final_answer.answer}")
        print(f"Source IDs Used: {trajectory.final_answer.source_ids}")
    else:
        print("No final answer provided by the agent")

    print(f"\nExpected Answer: {scenario.answer}")
    print(f"Expected Source IDs: {scenario.message_ids}")

