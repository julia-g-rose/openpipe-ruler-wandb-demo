"""
Helper functions and models for the email search agent training and evaluation.
"""
import json
import logging
import yaml
from dataclasses import asdict
from pathlib import Path
from textwrap import dedent

import weave
from langchain_core.utils.function_calling import convert_to_openai_tool
from litellm import acompletion
from openai import AsyncOpenAI
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt

import art
from art.utils.strip_logprobs import strip_logprobs
from enron_helpers import Scenario, FinalAnswer, search_emails, read_email


# Suppress weave validation errors
logging.getLogger("weave").setLevel(logging.CRITICAL)

# Decrease the number of turns from 10 to 6 to speed up training
MAX_TURNS = 6


def load_config(config_path: str = "config.yaml") -> dict:
    """
    Load configuration from YAML file.
    
    Args:
        config_path: Path to the YAML configuration file
        
    Returns:
        Dictionary containing the configuration
        
    Raises:
        FileNotFoundError: If the config file doesn't exist
    """
    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
    
    return config


class CorrectnessJudgeResponse(BaseModel):
    reasoning: str = Field(description="Explanation of the reasoning process.")
    accept: bool = Field(description="Whether the AI answer should be accepted.")


class CorrectnessJudgeScorer(weave.Scorer):
    """Weave Scorer for judging answer correctness using an LLM judge.
    
    This scorer evaluates whether an AI-generated answer is correct by comparing it
    to a reference answer using a judge LLM model.
    
    Attributes:
        judge_model: The LLM model to use for judging (default: openai/gpt-4.1)
        max_retries: Maximum number of retry attempts (default: 3)
    """
    judge_model: str = "openai/gpt-4.1"
    max_retries: int = 3
    
    @weave.op()
    async def score(
        self, 
        output: str, 
        question: str,
        reference_answer: str
    ) -> dict:
        """Score the output by judging its correctness against a reference answer.
        
        Args:
            output: The AI-generated answer to evaluate
            question: The original question being answered
            reference_answer: The reference/ground truth answer
            
        Returns:
            Dict containing:
                - correct: Boolean score (1.0 if accepted, 0.0 if rejected)
                - reasoning: Explanation of the judge's decision
                - accept: Whether the answer should be accepted
        """
        @retry(stop=stop_after_attempt(self.max_retries))
        async def _judge_with_retry():
            system_prompt = dedent(
                """
                You are given a question, the reference answer (labelled **Reference answer**), and an answer generated by an AI assistant (labelled **AI answer**).

                Your task is to decide whether the AI answer is correct and should be accepted. You should accept the answer if it contains the relevant information from the reference answer. You should not accept the answer if it is missing information relevant to the question, or if it contradicts the reference answer.
                """
            )

            messages = [
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": (
                        f"Question: {question}\n"
                        f"Reference answer: {reference_answer}\n"
                        f"AI answer: {output}"
                    ),
                },
            ]

            response = await acompletion(
                model=self.judge_model,
                messages=messages,
                response_format=CorrectnessJudgeResponse,
            )

            first_choice = response.choices[0]
            raw_content = first_choice.message.content or "{}"

            try:
                return CorrectnessJudgeResponse.model_validate_json(raw_content)
            except Exception as e:
                return CorrectnessJudgeResponse(
                    reasoning=f"Parse error: {e}\nRaw: {raw_content}", accept=False
                )
        
        try:
            judge_response = await _judge_with_retry()
            return {
                "correct": float(judge_response.accept),
                "reasoning": judge_response.reasoning,
                "accept": judge_response.accept
            }
        except Exception as e:
            # If all retries fail, return a failed judgment
            return {
                "correct": 0.0,
                "reasoning": f"Failed to get judgment after {self.max_retries} attempts: {str(e)}",
                "accept": False
            }


class SourceRetrievalScorer(weave.Scorer):
    """Weave Scorer for evaluating whether the agent retrieved the correct source emails.
    
    This scorer checks if the agent's final answer references the correct source
    email IDs that were expected for the scenario. It evaluates the quality of
    the agent's search and retrieval strategy.
    
    Metrics returned:
        - source_precision: Precision of retrieved sources (correct / total retrieved)
        - source_recall: Recall of retrieved sources (correct / total expected)
        - source_f1: F1 score combining precision and recall
        - retrieved_correct_sources: Whether all expected sources were retrieved
    """
    
    @weave.op()
    async def score(
        self,
        output: dict,  # Should contain 'source_ids' key
        expected_source_ids: list[str]
    ) -> dict:
        """Score the source retrieval quality.
        
        Args:
            output: Dict containing the agent's output, must have 'source_ids' key
            expected_source_ids: List of expected message IDs that should be retrieved
            
        Returns:
            Dict containing precision, recall, F1, and correctness metrics
        """
        # Extract source IDs from output
        if isinstance(output, dict):
            retrieved_ids = set(output.get('source_ids', []))
        elif hasattr(output, 'source_ids'):
            retrieved_ids = set(output.source_ids)
        else:
            # No sources retrieved
            retrieved_ids = set()
        
        expected_ids = set(expected_source_ids)
        
        # Calculate metrics
        if len(retrieved_ids) == 0:
            precision = 0.0
        else:
            correct_retrievals = retrieved_ids.intersection(expected_ids)
            precision = len(correct_retrievals) / len(retrieved_ids)
        
        if len(expected_ids) == 0:
            recall = 1.0 if len(retrieved_ids) == 0 else 0.0
        else:
            correct_retrievals = retrieved_ids.intersection(expected_ids)
            recall = len(correct_retrievals) / len(expected_ids)
        
        # F1 score
        if precision + recall == 0:
            f1 = 0.0
        else:
            f1 = 2 * (precision * recall) / (precision + recall)
        
        # Check if all expected sources were retrieved (perfect recall)
        retrieved_all = recall == 1.0
        
        return {
            "source_precision": precision,
            "source_recall": recall,
            "source_f1": f1,
            "retrieved_correct_sources": float(retrieved_all)
        }


class ToolUsageScorer(weave.Scorer):
    """Weave Scorer for evaluating the quality and efficiency of tool usage.
    
    This scorer analyzes the agent's tool call sequence to evaluate whether it:
    - Used tools appropriately and efficiently
    - Made necessary searches and email reads
    - Avoided excessive or redundant tool calls
    - Successfully completed with a final answer
    
    Available tools: search_inbox, read_email, return_final_answer
    
    Metrics returned:
        - total_tool_calls: Total number of tool calls made
        - search_calls: Number of search_inbox calls
        - read_calls: Number of read_email calls  
        - completed_task: Whether return_final_answer was called (1.0 or 0.0)
        - tool_efficiency: Efficiency score (1.0 / (1 + excess_calls))
        - used_search: Whether search was used at least once (1.0 or 0.0)
    """
    
    max_expected_calls: int = 10  # Configurable threshold for "efficient" usage
    
    @weave.op()
    async def score(
        self,
        trajectory: dict  # Should contain 'messages_and_choices' key with tool call info
    ) -> dict:
        """Score the tool usage quality from a trajectory.
        
        Args:
            trajectory: Dict containing the agent's trajectory with messages and tool calls
            
        Returns:
            Dict containing tool usage metrics
        """
        # Extract messages from trajectory
        if isinstance(trajectory, dict):
            messages = trajectory.get('messages_and_choices', [])
        elif hasattr(trajectory, 'messages_and_choices'):
            messages = trajectory.messages_and_choices
        else:
            messages = []
        
        # Count tool calls by type
        search_calls = 0
        read_calls = 0
        completed = False
        total_tool_calls = 0
        
        for msg in messages:
            # Check if this is an assistant message with tool calls
            if isinstance(msg, dict):
                tool_calls = msg.get('tool_calls', [])
                if tool_calls:
                    for tool_call in tool_calls:
                        total_tool_calls += 1
                        tool_name = tool_call.get('function', {}).get('name', '')
                        if tool_name == 'search_inbox':
                            search_calls += 1
                        elif tool_name == 'read_email':
                            read_calls += 1
                        elif tool_name == 'return_final_answer':
                            completed = True
            # Handle message objects with attributes
            elif hasattr(msg, 'message') and hasattr(msg.message, 'tool_calls'):
                tool_calls = msg.message.tool_calls or []
                for tool_call in tool_calls:
                    total_tool_calls += 1
                    tool_name = tool_call.function.name
                    if tool_name == 'search_inbox':
                        search_calls += 1
                    elif tool_name == 'read_email':
                        read_calls += 1
                    elif tool_name == 'return_final_answer':
                        completed = True
        
        # Calculate efficiency (penalize excessive tool calls)
        # Efficiency is 1.0 if within expected range, decreases with more calls
        if total_tool_calls <= self.max_expected_calls:
            efficiency = 1.0
        else:
            excess_calls = total_tool_calls - self.max_expected_calls
            efficiency = 1.0 / (1 + (excess_calls / self.max_expected_calls))
        
        # Check if essential tools were used
        used_search = search_calls > 0
        
        return {
            "total_tool_calls": float(total_tool_calls),
            "search_calls": float(search_calls),
            "read_calls": float(read_calls),
            "completed_task": float(completed),
            "tool_efficiency": efficiency,
            "used_search": float(used_search)
        }


class ProjectTrajectory(art.Trajectory):
    final_answer: FinalAnswer | None = None


class EmailScenario(BaseModel):
    step: int
    scenario: Scenario


@weave.op
async def rollout(
    model: art.Model, 
    email_scenario: EmailScenario,
    correctness_judge_model: str = "openai/gpt-4.1"
) -> ProjectTrajectory:
    """
    Execute a rollout of the email search agent on a given scenario.
    
    Args:
        model: The ART model to use for inference
        email_scenario: The email scenario to process
        correctness_judge_model: The model to use for judging answer correctness
        
    Returns:
        A ProjectTrajectory containing the agent's conversation and results
    """
    scenario = email_scenario.scenario

    traj = ProjectTrajectory(
        reward=0.0,
        messages_and_choices=[],
        metadata={
            "scenario_id": scenario.id,
            "step": email_scenario.step,
        },
    )

    system_prompt = dedent(
        f"""
        You are an email search agent. You are given a user query and a list of tools you can use to search the user's email. Use the tools to search the user's emails and find the answer to the user's query. You may take up to {MAX_TURNS} turns to find the answer, so if your first search doesn't find the answer, you can try with different keywords.

        User's email address is {scenario.inbox_address}
        Today's date is {scenario.query_date}
        """
    )

    traj.messages_and_choices = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": scenario.question},
    ]

    def search_inbox(keywords: list[str]) -> list[dict]:
        """Search the inbox for emails matching the given keywords and return
        a list of dictionaries so the LLM can easily consume them."""
        results = search_emails(
            inbox=scenario.inbox_address,
            keywords=keywords,
            sent_before=scenario.query_date,
        )
        return [asdict(result) for result in results]

    def return_final_answer(
        answer: str, reference_message_ids: list[str]
    ) -> FinalAnswer:
        """Return the final answer and the message IDs of the emails that were used to generate the answer."""
        return FinalAnswer(answer=answer, source_ids=reference_message_ids)

    tools = [search_inbox, read_email, return_final_answer]
    tools_by_name = {t.__name__: t for t in tools}
    traj.tools = [convert_to_openai_tool(t) for t in tools]

    client = AsyncOpenAI(
        base_url=model.inference_base_url,
        api_key=model.inference_api_key,
    )

    for _ in range(MAX_TURNS):
        response = await client.chat.completions.create(
            model=model.get_inference_name(),
            temperature=1,
            messages=traj.messages(),
            tools=traj.tools,
        )

        response_message = response.choices[0].message
        traj.messages_and_choices.append(response.choices[0])

        if not response_message.tool_calls:
            return traj

        try:
            for tool_call in response_message.tool_calls:
                tool_name: str = tool_call.function.name
                if tool_name in tools_by_name:
                    tool_args = json.loads(tool_call.function.arguments)
                    tool_to_call = tools_by_name[tool_name]
                    result = tool_to_call(**tool_args)
                    traj.messages_and_choices.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "name": tool_name,
                            "content": str(result),
                        }
                    )

                    if tool_name == "return_final_answer":
                        traj.final_answer = result
                        # Score the trajectory using Weave scorers
                        if traj.final_answer:
                            # Score answer correctness
                            correctness_scorer = CorrectnessJudgeScorer(judge_model=correctness_judge_model)
                            correctness_result = await correctness_scorer.score(
                                output=traj.final_answer.answer,
                                question=scenario.question,
                                reference_answer=scenario.answer
                            )
                            traj.metrics["correct"] = correctness_result["correct"]
                            traj.metrics["reasoning"] = correctness_result["reasoning"]
                            
                            # Score source retrieval quality
                            source_scorer = SourceRetrievalScorer()
                            source_result = await source_scorer.score(
                                output={"source_ids": traj.final_answer.source_ids},
                                expected_source_ids=scenario.message_ids
                            )
                            traj.metrics["source_precision"] = source_result["source_precision"]
                            traj.metrics["source_recall"] = source_result["source_recall"]
                            traj.metrics["source_f1"] = source_result["source_f1"]
                            traj.metrics["retrieved_correct_sources"] = source_result["retrieved_correct_sources"]
                            
                            # Score tool usage quality
                            tool_scorer = ToolUsageScorer(max_expected_calls=MAX_TURNS * 2)
                            tool_result = await tool_scorer.score(
                                trajectory={"messages_and_choices": traj.messages_and_choices}
                            )
                            traj.metrics["total_tool_calls"] = tool_result["total_tool_calls"]
                            traj.metrics["search_calls"] = tool_result["search_calls"]
                            traj.metrics["read_calls"] = tool_result["read_calls"]
                            traj.metrics["completed_task"] = tool_result["completed_task"]
                            traj.metrics["tool_efficiency"] = tool_result["tool_efficiency"]
                            traj.metrics["used_search"] = tool_result["used_search"]
                        return traj
        except Exception as e:
            print(f"Error executing tool call: {e}")
            return traj

    # Score tool usage even if task wasn't completed (ran out of turns)
    tool_scorer = ToolUsageScorer(max_expected_calls=MAX_TURNS * 2)
    tool_result = await tool_scorer.score(
        trajectory={"messages_and_choices": traj.messages_and_choices}
    )
    traj.metrics["total_tool_calls"] = tool_result["total_tool_calls"]
    traj.metrics["search_calls"] = tool_result["search_calls"]
    traj.metrics["read_calls"] = tool_result["read_calls"]
    traj.metrics["completed_task"] = tool_result["completed_task"]
    traj.metrics["tool_efficiency"] = tool_result["tool_efficiency"]
    traj.metrics["used_search"] = tool_result["used_search"]
    
    return traj


def initialize_weave(project_name: str):
    """Initialize Weave with common settings."""
    weave.init(
        project_name,
        settings={"print_call_link": False},
        # remove logprobs before recording in Weave
        global_postprocess_output=strip_logprobs
    )


def print_trajectory(trajectory: ProjectTrajectory, scenario: Scenario):
    """
    Pretty print a trajectory's messages and final answer.
    
    Args:
        trajectory: The trajectory to print
        scenario: The original scenario for comparison
    """
    print("Agent's trajectory:")
    print("-" * 20)

    # Display the conversation
    messages = trajectory.messages()
    for i, msg in enumerate(messages):
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        tool_calls = msg.get("tool_calls", [])

        if role == "system":
            print(
                f"[SYSTEM]: {content[:100]}..."
                if len(content) > 100
                else f"[SYSTEM]: {content}"
            )
        elif role == "user":
            print(f"[USER]: {content}")
        elif role == "assistant":
            if tool_calls:
                print(f"[ASSISTANT]: {tool_calls}")
            if content:
                print(f"[ASSISTANT]: {content}")
        elif role == "tool":
            tool_name = msg.get("name", "unknown_tool")
            print(
                f"[TOOL - {tool_name}]: {content[:200]}..."
                if len(content) > 200
                else f"[TOOL - {tool_name}]: {content}"
            )

        print()

    print("-" * 50)
    if trajectory.final_answer:
        print(f"Agent's Final Answer: {trajectory.final_answer.answer}")
        print(f"Source IDs Used: {trajectory.final_answer.source_ids}")
    else:
        print("No final answer provided by the agent")

    print(f"\nExpected Answer: {scenario.answer}")
    print(f"Expected Source IDs: {scenario.message_ids}")

